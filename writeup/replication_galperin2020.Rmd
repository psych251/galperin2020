---
title: Replication Report for Study 2 by Galperin, Hahl, Stering, & Guo (2020, Administrative
  Science Quarterly)
author: "Jiwon Byun (jwbyun@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020")
```




## Introduction

Employers' selection of job applicants is an important labor market proesses for both employers and individuals. For organizations, access to high-quality workers is a source of competitive advantage. For individuals, access to certain jobs at certain timing can have a large impact on one's career trajectories. Job signaling is the most fundamental and important aspect of selection processes in labor market. Information asymmetries between employers and job applicants provide challenges for both ends. \
Most studies have focused on job signaling and applicant capabilities when trying to understand selection processes in labor markets. On the other hand, increasing number of studies suggest that selection decisions are affected not only by perceptions of applicantss' capability but also by perceptions of applicants' commitment to the organization. \
While previous literature investigated both the effect of perceptions of applicants' capability and the effect of perceptions of applicants' commitment on the likelihood of a candidate's being selected for a job, the relationship between the two dimensions had been understudied. In an attempt to shed light on such a gap in the literature, Galperin, Hahl, Sterling, and Guo (Gelperin et al. henceforth) conducted a series of online field experiments and published an article in ASQ in the year of 2020. (For reference, see `Links` section in the below.) \
Galperin et al. (2020) suggested that signals that influence perceptions of capability in labor markets may have discernable effects of the perceptions of a candidate's commitment, which also affects the likelihood of the candidate's being selected for the job. The paper included four studies, three of which were online field experiments. In this study, I plan to replicate Study 2. \


### Justification for choice of study

In the original paper, the authors conducted a direct test of the effect of applicants' commitment and high-capability rejection. 
Study 2 was conducted to directly manipulated perceived commitment of an applicant to complement one of the other studies where they could not directly manipulate commitment. This is important because the main contribution of the study is suggested (and proved) mediation effect of commitment on likelihood of a job offer. 



### Anticipated challenges

I anticipate two challenges while attempting to replicate the results. 
First, I may run into sample issue. This study is about job signals and hiring managers' decision making in making job offers. Hence, it is important that I only have hiring managers (defined by someone with experiences as a hiring manager) in the sample. Since I have to rely on Prolifics to do the sampling, less appropriate sample might be acquired in the end. Similar issue applies to random assignment of participants. 
Second, survey design might be difficult. The main part of this replication study is manipulating commitment of a job applicant. Since commitment could mean different things both conceptually and practically, it is essential that I manipulate the main concepts elaborately. To overcome this challenge, I plan to reach out to original authors and try to acquire survey items that they used for the original study.



### Links

* Project repository (on Github): [Link to project repository](https://github.com/psych251/galperin2020.git)
* Original paper (as hosted in your repo): [Link to original paper](https://github.com/psych251/galperin2020/blob/1176bad85fcced90757aa9a19be77c14f0b70a5f/original_paper/galperin_et_al_2020.pdf)


\


## Methods


### Description of the steps required to replicate the results

To replicate the results of Study 2, I plan to run the same online experiment on Prolific. Prior to running the main study, I plan to run two pilot studies. The first pilot study will involve non-naive participants, whereas the second pilot study will have naive participants on MTurk. \
For the main study, participants will be recruited to fill out a "survey for hiring managers" through Prolific in October-November 2021. \
The study will have the same structure as that of the original study: 2 by 2 between-subjects design. Participants will be randomly assigned to one of the four conditions. (2 capability conditions X 2 commitment conditions) They will be given the same stimuli as the original study. \
I plan to conduct a manipulation check for both candidate's perceived capability and perceived commitment by combining the questionnaires included in the survey. \
With the data acquired from the survey experiment, I plan to conduct two t-tests to replicate the key results of Study 2. As an exploratory analysis, I plan to conduct a two-way ANOVA to examine the interaction effects. \



### Power Analysis

The original study had a sample size of 212. For the first t-test (neutral commitment condition), the statistics were: `t = 3.66`, `p < 0.001`. For the second t-test (high commitment condition), the results were: `t = -3.34`, `p = 0.001`.

Since I have two t-test for the study, I chose the t-test with smaller t value and used the study for the power analysis, which is the second t-test with `t = -3.34`. Then I did a calculation assuming a balanced cell size.

Results of the effect size calculation for the second t-test (calculated by using mean and s.d.) were as follows: `Cohen's d = 0.6554`,  `effect-size r = 0.3114`. I conducted the effect size calculation using the t-value and degrees of freedom as well and obtained very similar results.

Based on the effect size, I conducted a power analysis. Results for the power analysis for 80%, 90%, 95% power (for the second t-test) are as follows:

* 80% power: 76
* 90% power: 100
* 95% power: 124

Since the original work did have two t-tests, I needed to take the results and multiply by 2 to get the results for power analysis for the whole study. Therefore, the final results for the power analysis were as follows:

* 80% power: 152
* 90% power: 200
* 95% power: 248

The sample size for the 80% power (based on the original effect of the second t-test) was 152. I could not have 152 participants due to some constraints (mainly financial.) The original study paid the participants high compensation (`$6.00`) for this cognitively engaging study, I want to provide a reasonable amount of compensation to each participant. Considering the approximate time needed to complete the study and the hourly pay, I decided to pay `$2.00` each. My budget is limited at about `$400.00` for the study, so I decided have a sample size of `150`.
\


### Planned Sample

The sample will consist of `150` participants recruited from Prolific. In order to participate in the study, individuals should have an experience hiring people for the organization because this study addresses job decision making processes of hiring managers in the labor market. 
\

### Materials

The survey experiment materials were coded in Qualtrics. Participants recuited on Prolific could click on the link to the survey on Qualtrics. Participants were randomly assigned to `2 capability conditions` that involved two different job candidate resumes. Participants were also randomly assigned to `2 commitment conditions` given by information from HR department. The procedure for the study is described below.


#### Link:

* Survey material: [Link to the survey](https://stanforduniversity.qualtrics.com/jfe/preview/SV_3egYJH4CkorsKzk?Q_CHL=preview&Q_SurveyVersionID=current)

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.
\


### Procedure 

"The procedure for Study 2 was very similar to that for Study 1. We used the same job candidate profiles as in Study 1, but instead of evaluating both candidates, participants viewed only one profile and were asked to evaluate only that candidate applying to a 'corporate finance position in a medium-size health care company.' To verify that participants understood the directions, we asked them a series of attention-check questions. If they answered incorrectly, the context was repeated to them as a reinforcement.

One additional difference between Study 1 and Study 2 was that in this experiment, participants were told that their job as 'line manager' meant they 'should not concern themselves with negotiating the offer, but should only consider this candidate's potential performance on the job.' This was repeated before the participant was asked about his or her likelihood of giving the candidate an offer. We also asked participants to rate from 1 (low) to 7 (high) how concerned they were about the candidate accepting the job. There was no difference across conditions in this measure, and results presented below are robust to controlling for this measure. This indicates that participants behaved as intended and focused on post-hire commitment rather than concerns about the candidateâ€™s acceptance of a job offer.

Participants were randomly assigned to view either the extremely highcapability candidate or the moderately high-capability candidate, who were presented in the same way as they were in Study 1. Next, participants were randomly assigned to one of two commitment-information conditions: neutral or high. All participants were told, 'As part of the application process, the candidate completed an assessment delivered by your firmâ€™s HR department,' that the candidate scored a 75 on the assessment, and that 'candidates that have scored at least 60 points have skills that match the requirements of the job.' In the neutral-commitment condition, nothing was mentioned about the assessmentâ€™s relationship with commitment. Participants were told that 'this assessment was able to predict the candidate's ability to do the job in your firm.' In contrast, participants randomly assigned to the high-commitment condition were told that 'this assessment was able to predict the candidate's likely commitment to your organization (e.g., likelihood to stay with your firm, be motivated and work well with others).' This procedure resulted in a 2 (extremely high capability/moderately high capability) by 2 (high commitment/neutral commitment) between-subject design.

Participants were then asked about their likelihood of giving the candidate a job offer based on the same scale used in Study 1. This variable served as the dependent variable." (Galperin et al., 2020)

For this replication, the original work's procedure (described in the quotation above) was followed precisely.


#### Manipulation Checks

"As a manipulation check after rating their likelihood to give the candidate an offer, participants were then asked to evaluate the candidateâ€™s capability by rating him on a 7-point scale (1 = low and 7 = high) on four criteria related to capability: â€˜â€˜how competent the candidate would be in his jobâ€™â€™; â€˜â€˜how productive the candidate was in his previous jobâ€™â€™; â€˜â€˜how skilled the candidate isâ€™â€™; and â€˜â€˜how much finance experience the candidate has.â€™â€™ We combined these four measures to create a measure for the candidateâ€™s perceived capability (Cronbachâ€™s alpha = .88). Participants randomly assigned to the extremely high-capability conditions (N = 106, mean = 5.86, S.D. = .82) rated the candidate higher in perceived capability (t = âˆ’ 6.60, d.f. = 210, p < .001, two-tailed test) than those assigned to the moderately high-capability condition (N = 106, mean = 4.99, S.D. = 1.08).

Participants were also asked about perceived commitment after these ratings. They were asked to rate (1 = low, 7 = high) the candidate on the following four dimensions: â€˜â€˜likely commitment to the organization,â€™â€™ â€˜â€˜commitment to the industry,â€™â€™ â€˜â€˜willingness to stay with the company,â€™â€™ and â€˜â€˜willingness to work extra hours.â€™â€™ We added commitment to the industry to the measure (compared with the Study 1 scale) to address the change in industry. These ratings were averaged to create a perceived organizational commitment score (Cronbachâ€™s alpha = .89). Participants randomly assigned to the highcommitment conditions (N = 105, mean = 5.44, S.D. = 1.16) rated the candidate higher in commitment (t = âˆ’ 5.94, d.f. = 210, p < .001, two-tailed test) than those assigned to the neutral-commitment conditions (N = 107, mean = 4.51, S.D. = 1.57)." (Galpein et al., 2020)

For this replication, the original work's procedure (described in the quotation above) for manipulation checks was followed precisely.



### Analysis Plan

Confirmatory statistical analysis is going to involve two t-tests, following the methods used in the original work. I run a t-test for High-commitment conditions, and another t-test for Neutral-commitment conditions. By doing so, I can test differential effects of extremely high capability on likelihood of getting an offer for different commitment conditions.

I plan to conduct the same manipulation check analyses of those from original work. There are 4 measures for capability and 4 measures for commitment. As the original authors did, I will average the ratings for capability and commitment (separately for each manipulation) and created a perceived capability measure and a perceived commitment measure. Then I will examine Cronbach's alpha value and conduct t-tests across different treatment groups.

Since the original work aims to explore the interactional effects of capability and commitment on a candidate's likelihood of getting an offer, I plan to conduct a two-wawy ANOVA as an exploratogy analysis. Two-way ANOVA can be a good statitical model since I am trying to examine the interation effect of candidate's capability and commitment (Capability X Commitment) on the likelihood of receiving an offer. By using the two-way factorial design, I can effectively examine the potential interaction effect between Capability and Commitment.


#### Pilot B and Sampling

While the first pilot (Pilot A) went smoothly, I had some issues with Pilot B. So I had to run Pilot B three times to identify the appropriate strategy to run the study on Prolific. Three pilots were conducted on Prolific between November 12, 2021 and November 14, 2021. All three pilots were conducted with sample size of N=4. The participants were recruited on Prolific. They had to have hiring experience to be eligible for the study.
The original plan was to recruit participants internationally on Prolific. After the first Pilot B, I realized that this sampling strategy was not appropriate. Even with the language filter, (fluency in English) international participants were taking too long to complete the study. The median value for the duration of the stuay was approximately 24 minutes for a 10-minute study.
Therefore I conducted the second Pilot B with different sampling strategy. I added another filter so that only residents of U.S. can participate in the study. I also deleted all the open-ended questions from the survey. After running the second pilot, I could see that the problem was more about participants' country of residence than having open-ended questions in the survey.
Hence, I ran the third Pilot B with the original survey design. The sampling strategy was the same as the second pilot (individuals with hiring experience who live in the U.S., fluent in English). The third pilot ran without any problem and I was able to collect data.
\



### Differences from Original Study 

The most important difference is that my study involves a different sample. First, the sample size will be different. The original work a sample size of N=212. My sample will have fewer participants than that of original paper. Since the study involves 2 x 2 design, such a difference in sample sizes may result in different statistical power.
Furthermore, the nature of the sample may vary. While the original work recruited participants through Qualtrics panel, I will recruit participants through Prolific. Substantial differences may exist when filtering for the target participants. The most important aspect of the sample is that the sample should include only individuals who served as hiring managers. For the original study, the participants were Qualtrics-verified hiring managers who were U.S. based. All the participants could provide evidence of having served as a hiring manager. On the other hand, I do not have a means of verifying participants' experience as hiring managers. I have to rely on self-report on Prolific. Hence, it is possible that the nature of my sample will be different from the original study.
Another important difference is timing. This research studies hiring in the labor market. With hiring, timing is very important because there are a lot of time sensitivities. Although the original work was published in 2020, Study 2 was conducted in December 2016. This replication is conducted in November 2021. Since there is 5-year time difference, how people think about hiring could have changed substantially over those 5 years, expecially since the outbreak of the COVID-19 pandemic.
I acknowledge that any unforeseen issues are the sole responsiblitity of mine.
\



### Methods Addendum (Post Data Collection)

#### Actual Sample

* Sample size: My final sample consists of N=137 participants. I originally planned to have a sample size of N=150. However, I could not recruit 150 participants due to the limited budget. I had to run multiple pilot studies as I encountered minor issues as I was conducting pilot B. As a result, I spent more than my original budget for the pilot studies. Therefore, I recruited as many participants as I could within my constrained budget, which resulted with a sample with 137 participants. (The original work had a sample of N=212.)

* Demographics: The final sample consists of individuals who currently reside in the U.S. The original plan was to recruit internationally. However, Pilot B raised concerns. Job candidate's qualification and evaluations depend on the nuiances in the labor market, which may be different across different regions. So I decided to limit the sample to residents of U.S. only, which follows the original work's approach since the original sample had hiring managers who were U.S. based.

* Data exclusions: Individuals who failed to check eligibility question were excluded from the final sample. As an eligibility check, the survey asked participants if they had any hiring experience in the beginning. Those who reported that they did not have any hiring experience were excluded from the study. Furthermore, individuals who entered the survey and dropped out voluntarily without completing the questionnaire were also excluded from the final sample.



#### Differences from pre-data collection methods plan

None.


\




## Results


### Data preparation

First, when the study is done, download data from Qualtrics using `qualtRics` package. (Other than `qualtRics` package, load other essential packages for data analysis and visualization, such as: `tidyverse`, `knitr`, `ggplot2`, `psych`, `stats`, and etc.) \
Second, clean the data. Check for missing values and exclude observations with missing values. If needed, conduct data manipulation. Clean and manipulate data so that I have a clean tidy data. \
Third, conduct descriptive analyses. Before running the main statistical models, run some descriptive analysis and show the descriptives as needed. \
Fourth, conduct main statistical analysis. Present the findings in a table. \
Fifth, do some data visualization so that the readers can view the results easily. \
Lastly, if needed, run some additional analysis. (Check for homoscedasticity, conduct robustness checks, more explaratory analysis, etc.) \
\



#### Load Relevant Libraries and Functions

```{r, echo = TRUE}
library(qualtRics)
library(tidyverse)
library(knitr)
library(psych)
library(haven)
library(Rmisc)
library(purrr)
library(broom)
library(rstatix)
library(ggpubr)
library(psych)
```

\

#### Import data

```{r, echo = TRUE}
# load survey
#data_fin_raw <- fetch_survey(surveyID = "SV_82M7ojjd1rLcGPA", force_request = T)
```

```{r, echo = TRUE}
# save data
#write_csv(data_fin_raw, "/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/data/data_fin_raw.csv")
```

```{r, echo = TRUE}
# load data from csv
data_fin_raw <- read_csv("/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/data/data_fin_raw.csv")
```

\

#### Data exclusion / filtering

```{r}
data_raw <- data_fin_raw
```

```{r}
# exclude participants who did not meet the eligibility requirement
# or those who might have been included by accident (i.e. myself) who did not go through Prolific
# 6 obs dropped
data <- data_raw %>%
  filter(hiringexperience == "Yes") %>%
  filter(!is.na(prolific_id))

describe(data)

# add id column
data <- tibble::rowid_to_column(data, "id")
```



#### Prepare data for analysis - create columns etc.


```{r}
### Clean data (first step)

# select needed columns
data <- data %>%
  dplyr::select(c('ResponseId', 'id', 'companychk', 'candatchk', 'Location', 'interview', 'offer', 'opencncrn', 'openresolv', 'referral', 'competent', 'productive', 'skilled', 'finexp', 'committed', 'commit_ind', 'extrhrs', 'teamwork', 'stay', 'flightrsk', 'oe_qual', 'oe_comm', 'oe_interview', 'oe_offer', 'oe_referral', 'ethnicity', 'race_1', 'race_2', 'race_3', 'race_4', 'race_5', 'race_6', 'race_7', 'race_8', 'race_8_TEXT', 'gender', 'Age', 'educ', 'workexperience', 'Profile_DO_high_overqual_profil', 'Profile_DO_low_overqual_profile', 'CommitmentManipulation_DO_highcommitment_HR', 'CommitmentManipulation_DO_neutralcommitment_HR'))


# data manipulation - scores

# likelihood of getting an interview (score)
data <- data %>%
  mutate(interview_score = ifelse(interview == "Very Unlikely", 1,
                                  ifelse(interview == "Unlikely", 2,
                                         ifelse(interview == "Somewhat Unlikely", 3,
                                                ifelse(interview == "Undecided", 4,
                                                       ifelse(interview == "Somewhat Likely", 5,
                                                              ifelse(interview == "Likely", 6, 
                                                                     ifelse(interview == "Very Likely", 7, NA))))))))

# likelihood of getting an offer (score)
data <- data %>%
  mutate(offer_score = ifelse(offer == "Very Unlikely", 1, 
                              ifelse(offer == "Unlikely", 2, 
                                     ifelse(offer == "Somewhat Unlikely", 3, 
                                            ifelse(offer == "Undecided", 4, 
                                                   ifelse(offer == "Somewhat Likely", 5, 
                                                          ifelse(offer == "Likely", 6,  
                                                                 ifelse(offer == "Very Likely", 7, NA))))))))


# referral or not
data <- data %>%
  mutate(refer = ifelse(referral == "Yes", 1, 0)) %>%
  mutate(refer = as.factor(refer))


### Manipulation checks

# competent
data <- data %>%
  mutate(competent_score = ifelse(competent == "Strongly disagree", 1, 
                                  ifelse(competent == "Disagree", 2, 
                                         ifelse(competent == "Somewhat disagree", 3, 
                                                ifelse(competent == "Neither agree nor disagree", 4, 
                                                       ifelse(competent == "Somewhat agree", 5, 
                                                              ifelse(competent == "Agree", 6, 
                                                                     ifelse(competent == "Strongly agree", 7, NA))))))))


# productive
data <- data %>%
  mutate(productive_score = ifelse(productive == "Strongly disagree", 1, 
                                   ifelse(productive == "Disagree", 2, 
                                          ifelse(productive == "Somewhat disagree", 3, 
                                                 ifelse(productive == "Neither agree nor disagree", 4, 
                                                        ifelse(productive == "Somewhat agree", 5, 
                                                               ifelse(productive == "Agree", 6,
                                                                      ifelse(productive == "Strongly agree", 7, NA))))))))

# skilled
data <- data %>%
  mutate(skilled_score = ifelse(skilled == "Strongly disagree", 1,
                                ifelse(skilled == "Disagree", 2,
                                       ifelse(skilled == "Somewhat disagree", 3,
                                              ifelse(skilled == "Neither agree nor disagree", 4,
                                                     ifelse(skilled == "Somewhat agree", 5,
                                                            ifelse(skilled == "Agree", 6,
                                                                   ifelse(skilled == "Strongly agree", 7, NA))))))))

# finexp
data <- data %>%
  mutate(finexp_score = ifelse(finexp == "Strongly disagree", 1,
                               ifelse(finexp == "Disagree", 2,
                                      ifelse(finexp == "Somewhat disagree", 3,
                                             ifelse(finexp == "Neither agree nor disagree", 4,
                                                    ifelse(finexp == "Somewhat agree", 5,
                                                           ifelse(finexp == "Agree", 6,
                                                                  ifelse(finexp == "Strongly agree", 7, NA))))))))

# committed
data <- data %>%
  mutate(commit_score = ifelse(committed == "Highly uncommitted", 1,
                               ifelse(committed == "Uncommitted", 2,
                                      ifelse(committed == "Somewhat uncommitted", 3,
                                             ifelse(committed == "Neither committed nor uncommitted", 4,
                                                    ifelse(committed == "Somewhat committed", 5,
                                                           ifelse(committed == "Committed", 6,
                                                                  ifelse(committed == "Highly committed", 7, NA))))))))

# commit_ind
data <- data %>%
  mutate(commit_ind_score = ifelse(commit_ind == "Highly uncommitted", 1,
                                   ifelse(commit_ind == "Uncommitted", 2,
                                          ifelse(commit_ind == "Somewhat uncommitted", 3,
                                                 ifelse(commit_ind == "Neither committed nor uncommitted", 4,
                                                        ifelse(commit_ind == "Somewhat committed", 5,
                                                               ifelse(commit_ind == "Committed", 6,
                                                                      ifelse(commit_ind == "Highly committed", 7, NA))))))))



# extra hours score
data <- data %>%
  mutate(extrhrs_score = ifelse(extrhrs == "Very unlikely", 1,
                                ifelse(extrhrs == "Unlikely", 2,
                                       ifelse(extrhrs == "Somewhat unlikely", 3,
                                              ifelse(extrhrs == "Neither likely nor unlikely", 4,
                                                     ifelse(extrhrs == "Somewhat likely", 5,
                                                            ifelse(extrhrs == "Likely", 6,
                                                                   ifelse(extrhrs == "Very likely", 7, NA))))))))

# teamwork score
data <- data %>%
  mutate(teamwork_score = ifelse(teamwork == "Very Unlikely", 1, 
                                 ifelse(teamwork == "Unlikely", 2, 
                                        ifelse(teamwork == "Somewhat unlikely", 3, 
                                               ifelse(teamwork == "Neither likely nor unlikely", 4, 
                                                      ifelse(teamwork == "Somewhat likely", 5, 
                                                             ifelse(teamwork == "Likely", 6, 
                                                                    ifelse(teamwork == "Very likely", 7, NA))))))))



# likelihood of staying score
data <- data %>%
  mutate(stay_score = ifelse(stay == "Less than 1 year", 1,
                             ifelse(stay == "1-2 years", 2,
                                    ifelse(stay == "2-3 years", 3,
                                           ifelse(stay == "3-4 years", 4,
                                                  ifelse(stay == "4-5 years", 5,
                                                         ifelse(stay == "5-6 years", 6,
                                                                ifelse(stay == "Greater than 6 years", 7, NA))))))))





# flight risk score (flightrsk)
data <- data %>%
  mutate(flight_score = ifelse(flightrsk == "Very unlikely", 1, 
                               ifelse(flightrsk == "Unlikely", 2,
                                      ifelse(flightrsk == "Somewhat unlikely", 3,
                                             ifelse(flightrsk == "Neither likely nor unlikely", 4, 
                                                    ifelse(flightrsk == "Somewhat likely", 5,
                                                           ifelse(flightrsk == "Likely", 6,
                                                                  ifelse(flightrsk == "Very likely", 7, NA))))))))

```

```{r}
### Data cleaning continuted - demographic information

#summary(as.factor(data$race_1))
#summary(as.factor(data$race_2))
#summary(as.factor(data$race_3))
#summary(as.factor(data$race_4))
#summary(as.factor(data$race_5))
##summary(as.factor(data$race_6))
#summary(as.factor(data$race_7))
##summary(as.factor(data$race_8))
##summary(as.factor(data$race_8_TEXT))


# ethnicity
data <- data %>%
  mutate(ethnic_hispanic = ifelse(is.na(ethnicity), 0, ifelse(ethnicity == "Hispanic", 1, 0)))

# race
data <- data %>%
  mutate(White = ifelse(!is.na(race_1), 1, 0)) %>%
  mutate(Black = ifelse(!is.na(race_2), 1, 0)) %>%
  mutate(Hispanic = ifelse(!is.na(race_3), 1, 0)) %>%
  mutate(Asian = ifelse(!is.na(race_4), 1, 0)) %>%
  mutate(Native = ifelse(!is.na(race_5), 1, 0)) %>%
  mutate(Decline = ifelse(!is.na(race_7), 1, 0))
# variables with no obs dropped (race_6, race_8, race_8_TEXT)

# create multiple race variable
data <- data %>%
  mutate(Race_sum = White + Black + Hispanic + Asian + Native + Decline)

data <- data %>%
  mutate(Race_multi = ifelse(Race_sum > 1, 1, 0))

# create one race variable (factor)
data <- data %>%
  mutate(race = ifelse(Race_multi == 1, "Multirace",
                       ifelse(White == 1, "White",
                              ifelse(Black == 1, "Black",
                                     ifelse(Hispanic == 1, "Hispanic",
                                            ifelse(Asian == 1, "Asian",
                                                   ifelse(Native == 1, "Native",
                                                          ifelse(Decline == 1, "Decline", NA)))))))) %>%
  mutate(race = as.factor(race))

# gender
data <- data %>%
  mutate(gender = as.factor(gender))

# education
data <- data %>%
  mutate(educ = as.factor(educ))


#summary(data$educ)

# add education level (ordinal variable)
# educ_ordinal
data <- data %>%
  mutate(educ_ordinal = ifelse(educ == "Less than High School", 1,
                               ifelse(educ == "High School / GED", 2,
                                      ifelse(educ == "Some College", 3,
                                             ifelse(educ == "2-year College Degree", 4,
                                                    ifelse(educ == "4-year College Degree", 5,
                                                           ifelse(educ == "Masters Degree", 6,
                                                                  ifelse(educ == "Doctoral Degree", 7,
                                                                         ifelse(educ == "Professional Degree (JD, MD)", 8, NA)))))))))
```




```{r}
# Treatment conditions

# Overqual condition (Capability)
data <- data %>%
  mutate(Overqual = ifelse(is.na(Profile_DO_high_overqual_profil), "Low", "High")) %>%
  mutate(Overqual = as.factor(Overqual))

data <- data %>%
  mutate(Commitment = ifelse(is.na(CommitmentManipulation_DO_highcommitment_HR), "Neutral", "High")) %>%
  mutate(Commitment = as.factor(Commitment))



```

```{r}
# Before selecting out variables, let's save this version as a csv file
#write.csv(data, "/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/data/data_fin_wrangled.csv", row.names = F)
```


```{r}
# clean data (select needed columns only - exclude open-ended questions as well)
data_clean <- data %>%
  dplyr::select(c('id', 'Commitment', 'Overqual', 'offer_score', 'interview_score', 'refer', 'competent_score', 'productive_score', 'skilled_score', 'finexp_score', 'commit_score', 'commit_ind_score', 'extrhrs_score', 'teamwork_score', 'stay_score', 'flight_score', 'gender', 'Age', 'ethnic_hispanic', 'race', 'educ_ordinal', 'workexperience', 'White', 'Black', 'Hispanic', 'Asian', 'Native', 'Decline', 'educ'))


data_clean <- data_clean %>%
  mutate(Commitment = factor(Commitment, levels = c("Neutral", "High"), labels = c("Neutral", "High"))) %>%
  mutate(Overqual = factor(Overqual, levels = c("Low", "High"), labels = c("Low", "High")))

# save the clean data
#write.csv(data_clean, "/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/data/data_fin_clean.csv", row.names = F)
```


```{r}
# Data with neutral commitment condition (for t-test)
data_neutral_commit <- data_clean %>%
  filter(Commitment == "Neutral")

# Data with high commitment condition (fot t-test)
data_high_commit <- data_clean %>%
  filter(Commitment == "High")
```



### Descriptive statistics

```{r}
# Check number of participants in each condition
kable(table(data$Overqual, data$Commitment), caption = "Number of participants in each condition")
```


```{r}
kable(describe(data_clean), caption = "Overall Descriptive Statistics")
```

```{r}
desc_tab <- data_clean %>%
  dplyr::group_by(Commitment, Overqual) %>%
  dplyr::summarise(Age = mean(Age, na.rm = T),
            WorkExperience = mean(workexperience, na.rm = T),
            Offer = mean(offer_score),
            Interview = mean(interview_score),
            Educ_level = mean(educ_ordinal, na.rm = T))

kable(desc_tab, caption = "Key Descriptive Statistics for Each Condition")

```

```{r}
summ_mean_sd <- data_clean %>%
  group_by(Commitment, Overqual) %>%
  get_summary_stats(offer_score, type = "mean_sd")
kable(summ_mean_sd, caption = "Mean and s.d. of Likelihood of Offer across All Conditions")
```



```{r}
ggplot(data = data_clean, aes(x = Age)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(col = "blue") +
  facet_grid(Overqual ~ Commitment) +
  theme_classic() +
  ylab("Density") +
  ggtitle("Histogram of participant age in each condition")

```
```{r}
ggplot(data = data_clean, aes(x = workexperience)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(col = "blue") +
  facet_grid(Overqual ~ Commitment) +
  theme_classic() +
  ylab("Density") +
  xlab("Work Experience (in years)") +
  ggtitle("Histogram of participant work experience in each condition")
```

```{r}
ggplot(data = data_clean, aes(x = offer_score)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(col = "blue") +
  theme_classic() +
  facet_wrap(~ Commitment) +
  ylab("Density") +
  xlab("Likelihood of Offer") +
  ggtitle("Likelihood of getting an offer in each commitment condition")
```


```{r}
plot_box <- ggboxplot(
  data = data_clean, x = "Overqual", y = "offer_score",
  color = "Commitment", palette = "jco"
)

plot_box
```



```{r}
commit_labels <- c("Neutral Commitment", "High Commitment")
names(commit_labels) <- c("Neutral", "High")

ggplot(data = data_clean, aes(x = Overqual, y = offer_score, group = Overqual, fill = Overqual)) +
  geom_boxplot(alpha = 0.6) +
  geom_jitter(width = 0.1, alpha = 0.6) +
  guides(fill = "none") +
  scale_y_continuous(breaks = 1:7, limits = c(1, 7)) +
  theme(legend.position = c(0.9, 0.15),
        legend.direction = "vertical",
        legend.background = element_rect(fill = "transparent"),
        legend.title = element_blank(),
        axis.line = element_line(),
        panel.grid = element_blank(), 
        panel.background = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  facet_wrap(~Commitment, labeller = labeller(Commitment = commit_labels)) +
  ggtitle("Final Results (N = 137)") +
  labs(x = "Capability", y = "Likelihood of an Offer")

#ggsave("/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/data/final_data_boxplot.png")
```





#### Manipulation check

```{r}
# Capability manipulatio check (Overqual)
require(dplyr)
require(psych)

capa_check <- data_clean %>%
  dplyr::select(c('competent_score', 'productive_score', 'skilled_score', 'finexp_score'))

alpha(capa_check)

```




```{r}
# run t-test to examine if perceived capabilities are different across conditions
capa_check_t <- data_clean %>%
  dplyr::select(c('Overqual', 'competent_score', 'productive_score', 'skilled_score', 'finexp_score'))

capa_check_t$mean_capa <- rowMeans(capa_check_t[2:5], na.rm = T)

capa_test <- t.test(mean_capa ~ Overqual, data = capa_check_t, paired = F, var.equal = T)
capa_test

require(broom)

tidy(t.test(data = capa_check_t, mean_capa ~ Overqual, paired = F, var.equal = T))
```




```{r}
# Commitment manipulation check (Commitment)
commit_check <- data_clean %>%
  dplyr::select(c('commit_score', 'commit_ind_score', 'stay_score', 'extrhrs_score'))

alpha(commit_check)
```

```{r}
# run t-test to examine if perceived commitments are different across conditions
commit_check_t <- data_clean %>%
  dplyr::select(c('Commitment', 'commit_score', 'commit_ind_score', 'stay_score', 'extrhrs_score'))

commit_check_t$mean_commit <- rowMeans(capa_check_t[2:5], na.rm = T)

commit_test <- t.test(mean_commit ~ Commitment, data = commit_check_t, paired = F, var.equal = T)
commit_test

require(broom)

tidy(t.test(mean_commit ~ Commitment, data = commit_check_t, paired = F, var.equal = T))
```


I precisely followed the original work's manipulation check procedure. I combined the four measures used in the original work to create a measure for candidate's perceived capability. (Cronbach's alpha = 0.81) While the overall Cronbach's alpha value showed the consistency among the four measures was good, the participants randomly assigned to the extremely high-capability conditions did not rate the candidate higher in perceived capability than those assigned to the moderately high-capability condition. The t-test results were significant only at alpha = 0.1 level. (t = -1.96, d.f. = 135, p-value = 0.0518)
I combined four commitment measures used in the original work to create a perceived organizational commitment score. (Cronbach's alpha = 0.74) The overall Cronbach's alpha value fell under acceptable range. However, the commitment manipulation did not seem to be effective for the participants in this study. Participants randomly assigned to the high-commitment conditions did not rate the candidate higher in commitment than those assinged to the neutral-commitment conditions. (t = -1.23, d.f. = 135, p-value = 0.219)
The fact that manipulations were not effective (especially commitment conditions) to the participants in this replication study may suggest that the sample used for this study was substantially different than the original study's sample. The original work's manipulations were very effective. (All manipulation checks were significant with p < 0.001).

\

### Confirmatory analysis

I ran two t-tests: one for neutral commitment condition, the other for high commitment condition. The choice of key statistical model is determined by the original work, as the original paper ran two t-tests.


#### T-test for neutral-commitment conditions

```{r}
t.test(offer_score ~ Overqual, data = data_neutral_commit, paired = F, var.equal = T)

require(broom)
tidy(t.test(data = data_neutral_commit, offer_score ~ Overqual, paired = F, var.equal = T))
```

I found no statistically significant results from the first t-test (neutral-commitment condition). While the mean value of likelihood of getting an offer for the Extremely-high capability condition (mean = 5.65) was greater than that for the Moderately-high contition (mean = 5.24), the t-test results were not significant. (t = -1.48, df = 62, p-value = 0.14)


#### T-test for high-commitment condition

```{r}
t.test(offer_score ~ Overqual, data = data_high_commit, paired = F, var.equal = T)

require(broom)
tidy(t.test(data = data_high_commit, offer_score ~ Overqual, paired = F, var.equal = T))
```

I found no statistically significant results from the second t-test (high-commitment condition). While the mean value of likelihood of getting an offer for the Extremely-high capability condition (mean = 5.65) was greater than that for the Moderately-high contition (mean = 5.56), the t-test results were not significant. (t = -0.35, df = 71, p-value = 0.73)

\

#### Summary: Capability x Commitment and Likelihood of receiving an offer across all four conditions

```{r}
summ_mean_sd
```

```{r}
ggplot(data = summ_mean_sd, aes(x = Commitment, y = mean,
                                ymin = mean - sqrt(sd), ymax = mean + sqrt(sd), fill = Overqual)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_errorbar(position=position_dodge(0.9), width = .2) +
  scale_y_continuous(breaks = 1:7) +
  theme(legend.direction = "vertical",
        legend.position = "right",
        axis.line = element_line(),
        panel.grid.major.x = element_blank(),
        panel.grid.major.y = element_line(color = "grey", size = 0.15),
        panel.background = element_blank(),
        plot.title = element_text(hjust = 0.5)) +
  labs(x = "Commitment", y = "Likelihood of Offer", fill = "Capability") +
  ggtitle("Capability x Commitment: Likelihood of Offer across all Four Conditions")
```



> For reference, the original work's corresponding figure:
![Alt text](/Users/jbyun/Dropbox/2021 Fall/PSYCH 251/galperin2020/original_paper/original_figure.png)

\

### Exploratory analyses


#### Two-way ANOVA

```{r}
# Check normality assumption (QQ plot)
#ggqqplot(data_clean, "offer_score", ggtheme = theme_bw()) +
#  facet_grid(Overqual ~ Commitment)
```

```{r}
result_aov <- data_clean %>% anova_test(offer_score ~ Overqual*Commitment)
result_aov
```

The two-way ANOVA results show that there was no statitically significant interaction between Capability and Commitment condition for the likelihood of getting an offer.

\

#### Two-way ANCOVA

Since the results of t-tests and two-way ANOVA were not statistically significant, I examined the possibility of other factors, such as work experience and age, interfering with the results.

##### Check linearity assumption

```{r}
ggscatter(
  data = data_clean, x = "workexperience", y = "offer_score",
  facet.by = c("Commitment", "Overqual"),
  short.panel.labs = F,
  xlab = c("Work Experience (in years)"),
  ylab = c("Likelihood of Offer"),
) +
  stat_smooth(method = "loess", span = 0.9)
```

##### Homogeneity of regression slopes

This assumption checks that there is not significant interaction betwen the covariate and the grouping variables.

```{r}
data_clean %>%
  anova_test(
    offer_score ~ workexperience + Overqual + Commitment + Overqual*Commitment + workexperience*Overqual + workexperience*Commitment + workexperience*Commitment*Overqual
  )
```

I already know that there is no statistically significant interaction effect between Capability (Overqual) and Commitment. However, just as a sanity check, I ran another ANOVA to show that there is no significant interaction between Capability (Overqual) and Commitment here.
\

##### Interaction effect after controlling for participant's work experience (ANCOVA)

```{r}
work_aov <- data_clean %>%
  anova_test(offer_score ~ workexperience + Overqual*Commitment)

get_anova_table(work_aov)
```

There was no statistically significant interaction between Capability and Commitment on the likelihood of getting an offer after controlling for participant's work experience (in years).

\

##### Interaction effect after controlling for participant's age (ANCOVA)

```{r}
age_aov <- data_clean %>%
  anova_test(offer_score ~ Age + Overqual*Commitment)

get_anova_table(age_aov)
```

After adjusting for participant's age, I found no statistically significant interaction between Capability and Commitment on the likelihood of getting an offer.

\



> FILL IN THE BELOW

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
